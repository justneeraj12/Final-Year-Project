{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('49_updated.csv')\n",
    "df = df.drop('TIME', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>o</th>\n",
       "      <th>group_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107.136536</td>\n",
       "      <td>100.0</td>\n",
       "      <td>134.133804</td>\n",
       "      <td>2.854385</td>\n",
       "      <td>576.762146</td>\n",
       "      <td>99.946281</td>\n",
       "      <td>44.625851</td>\n",
       "      <td>46.853638</td>\n",
       "      <td>104.717056</td>\n",
       "      <td>106.523125</td>\n",
       "      <td>107.030479</td>\n",
       "      <td>131.141632</td>\n",
       "      <td>1.689493</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105.134583</td>\n",
       "      <td>100.0</td>\n",
       "      <td>160.002411</td>\n",
       "      <td>3.367386</td>\n",
       "      <td>510.683624</td>\n",
       "      <td>107.427765</td>\n",
       "      <td>54.109188</td>\n",
       "      <td>-0.692750</td>\n",
       "      <td>108.977722</td>\n",
       "      <td>97.884750</td>\n",
       "      <td>98.325340</td>\n",
       "      <td>135.435562</td>\n",
       "      <td>1.835044</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158.749390</td>\n",
       "      <td>100.0</td>\n",
       "      <td>177.491074</td>\n",
       "      <td>4.161911</td>\n",
       "      <td>489.616302</td>\n",
       "      <td>111.045418</td>\n",
       "      <td>51.477051</td>\n",
       "      <td>-0.692750</td>\n",
       "      <td>112.985031</td>\n",
       "      <td>127.458191</td>\n",
       "      <td>127.965530</td>\n",
       "      <td>138.692169</td>\n",
       "      <td>2.040076</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.557755</td>\n",
       "      <td>100.0</td>\n",
       "      <td>230.250870</td>\n",
       "      <td>6.420364</td>\n",
       "      <td>500.774811</td>\n",
       "      <td>120.365662</td>\n",
       "      <td>55.490112</td>\n",
       "      <td>-0.585938</td>\n",
       "      <td>118.745735</td>\n",
       "      <td>120.074837</td>\n",
       "      <td>120.488739</td>\n",
       "      <td>158.166794</td>\n",
       "      <td>2.533844</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207.734665</td>\n",
       "      <td>100.0</td>\n",
       "      <td>236.822556</td>\n",
       "      <td>6.057510</td>\n",
       "      <td>497.256439</td>\n",
       "      <td>124.775787</td>\n",
       "      <td>56.008911</td>\n",
       "      <td>-0.585938</td>\n",
       "      <td>125.489731</td>\n",
       "      <td>119.607536</td>\n",
       "      <td>120.061485</td>\n",
       "      <td>156.594208</td>\n",
       "      <td>2.461201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            a      b           c         d           e           f          g  \\\n",
       "0  107.136536  100.0  134.133804  2.854385  576.762146   99.946281  44.625851   \n",
       "1  105.134583  100.0  160.002411  3.367386  510.683624  107.427765  54.109188   \n",
       "2  158.749390  100.0  177.491074  4.161911  489.616302  111.045418  51.477051   \n",
       "3  218.557755  100.0  230.250870  6.420364  500.774811  120.365662  55.490112   \n",
       "4  207.734665  100.0  236.822556  6.057510  497.256439  124.775787  56.008911   \n",
       "\n",
       "           h           i           j           k           l         m  o  \\\n",
       "0  46.853638  104.717056  106.523125  107.030479  131.141632  1.689493  1   \n",
       "1  -0.692750  108.977722   97.884750   98.325340  135.435562  1.835044  1   \n",
       "2  -0.692750  112.985031  127.458191  127.965530  138.692169  2.040076  1   \n",
       "3  -0.585938  118.745735  120.074837  120.488739  158.166794  2.533844  1   \n",
       "4  -0.585938  125.489731  119.607536  120.061485  156.594208  2.461201  1   \n",
       "\n",
       "   group_status  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.20.3)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.6.2)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llava 1.1.3 requires pydantic<2,>=1, but you have pydantic 2.9.2 which is incompatible.\n",
      "videollava 1.0.0 requires gradio==3.37.0, but you have gradio 3.35.2 which is incompatible.\n",
      "videollava 1.0.0 requires gradio-client==0.7.0, but you have gradio-client 0.2.9 which is incompatible.\n",
      "videollava 1.0.0 requires pydantic<2,>=1, but you have pydantic 2.9.2 which is incompatible.\n",
      "datasets 2.16.1 requires dill<0.3.8,>=0.3.0, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.26.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justneeraj12/miniconda3/envs/ai_shit/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2854\n",
      "Epoch 2/10, Loss: 0.2432\n",
      "Epoch 3/10, Loss: 0.2346\n",
      "Epoch 4/10, Loss: 0.2294\n",
      "Epoch 5/10, Loss: 0.2238\n",
      "Epoch 6/10, Loss: 0.2224\n",
      "Epoch 7/10, Loss: 0.2190\n",
      "Epoch 8/10, Loss: 0.2176\n",
      "Epoch 9/10, Loss: 0.2171\n",
      "Epoch 10/10, Loss: 0.2150\n",
      "Test Accuracy: 0.8936, F1 Score: 0.9348\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('49_updated.csv')\n",
    "features = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'k', 'm']\n",
    "target = 'group_status'\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Prepare sequences of 10 rows\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(data, sequence_length, target_col):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[features].iloc[i:i+sequence_length].values\n",
    "        label = data[target_col].iloc[i + sequence_length - 1]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequences, labels = create_sequences(data, sequence_length, target)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(sequences))\n",
    "train_sequences, test_sequences = sequences[:split_index], sequences[split_index:]\n",
    "train_labels, test_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "# Custom Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_sequences, train_labels)\n",
    "test_dataset = TimeSeriesDataset(test_sequences, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Transformer model with an embedding layer to adjust input dimensions\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, n_heads, hidden_dim, n_layers):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        # Embedding layer to adjust input dimension\n",
    "        self.embedding = nn.Linear(input_dim, emb_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(emb_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Adjust to emb_dim\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = len(features)\n",
    "emb_dim = 16  # Choose an embedding dimension divisible by n_heads\n",
    "n_heads = 4\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "model = TransformerClassifier(input_dim=input_dim, emb_dim=emb_dim, n_heads=n_heads, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            predictions.extend((outputs.squeeze() > 0.5).int().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Train and evaluate\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justneeraj12/miniconda3/envs/ai_shit/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2748\n",
      "Epoch 2/10, Loss: 0.2438\n",
      "Epoch 3/10, Loss: 0.2341\n",
      "Epoch 4/10, Loss: 0.2287\n",
      "Epoch 5/10, Loss: 0.2235\n",
      "Epoch 6/10, Loss: 0.2214\n",
      "Epoch 7/10, Loss: 0.2173\n",
      "Epoch 8/10, Loss: 0.2167\n",
      "Epoch 9/10, Loss: 0.2154\n",
      "Epoch 10/10, Loss: 0.2131\n",
      "Test Accuracy: 0.7652, F1 Score: 0.8661\n",
      "Model saved as transformer_classifier.pth\n",
      "Model loaded for prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justneeraj12/miniconda3/envs/ai_shit/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_19580/60703028.py:147: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('transformer_classifier.pth'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load and preprocess the training data\n",
    "data = pd.read_csv('49_updated.csv')\n",
    "features = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'k', 'm']\n",
    "target = 'group_status'\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Prepare sequences of 10 rows\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(data, sequence_length, target_col):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[features].iloc[i:i+sequence_length].values\n",
    "        label = data[target_col].iloc[i + sequence_length - 1]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequences, labels = create_sequences(data, sequence_length, target)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(sequences))\n",
    "train_sequences, test_sequences = sequences[:split_index], sequences[split_index:]\n",
    "train_labels, test_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "# Custom Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_sequences, train_labels)\n",
    "test_dataset = TimeSeriesDataset(test_sequences, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Transformer model with an embedding layer to adjust input dimensions\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, n_heads, hidden_dim, n_layers):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, emb_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(emb_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = len(features)\n",
    "emb_dim = 16\n",
    "n_heads = 4\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "model = TransformerClassifier(input_dim=input_dim, emb_dim=emb_dim, n_heads=n_heads, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            predictions.extend((outputs.squeeze() > 0.5).int().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Train and save the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)\n",
    "torch.save(model.state_dict(), 'transformer_classifier.pth')\n",
    "print(\"Model saved as transformer_classifier.pth\")\n",
    "\n",
    "# Function to load and predict on a new dataset\n",
    "def predict_new_data(file_path, model, scaler):\n",
    "    # Load and preprocess new data\n",
    "    new_data = pd.read_csv(file_path)\n",
    "    new_data[features] = scaler.transform(new_data[features])  # Normalize using the same scaler\n",
    "    \n",
    "    # Prepare sequences for prediction\n",
    "    new_sequences = []\n",
    "    for i in range(len(new_data) - sequence_length):\n",
    "        seq = new_data[features].iloc[i:i+sequence_length].values\n",
    "        new_sequences.append(seq)\n",
    "    new_sequences = np.array(new_sequences)\n",
    "\n",
    "    # Prepare DataLoader for new data\n",
    "    new_dataset = TimeSeriesDataset(new_sequences, np.zeros(len(new_sequences)))  # Use zeros as dummy labels\n",
    "    new_loader = DataLoader(new_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, _ in new_loader:\n",
    "            outputs = model(sequences)\n",
    "            predictions = (outputs.squeeze() > 0.5).int().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# Load the saved model\n",
    "model = TransformerClassifier(input_dim=input_dim, emb_dim=emb_dim, n_heads=n_heads, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "model.load_state_dict(torch.load('transformer_classifier.pth'))\n",
    "print(\"Model loaded for prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on new data: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Predict on a new CSV file\n",
    "new_predictions = predict_new_data('46.csv', model, scaler)\n",
    "print(\"Predictions on new data:\", new_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2689\n",
      "Epoch 2/10, Loss: 0.2183\n",
      "Epoch 3/10, Loss: 0.2092\n",
      "Epoch 4/10, Loss: 0.2066\n",
      "Epoch 5/10, Loss: 0.2036\n",
      "Epoch 6/10, Loss: 0.1995\n",
      "Epoch 7/10, Loss: 0.1954\n",
      "Epoch 8/10, Loss: 0.1934\n",
      "Epoch 9/10, Loss: 0.1884\n",
      "Epoch 10/10, Loss: 0.1853\n",
      "Test Accuracy: 0.9366, F1 Score: 0.9602\n",
      "Model saved as cnn_classifier.pth\n",
      "Model loaded for prediction.\n",
      "Predictions on new data: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load and preprocess the training data\n",
    "data = pd.read_csv('49_updated.csv')\n",
    "features = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'k', 'm']\n",
    "target = 'group_status'\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Prepare sequences of 10 rows\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(data, sequence_length, target_col):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[features].iloc[i:i+sequence_length].values\n",
    "        label = data[target_col].iloc[i + sequence_length - 1]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequences, labels = create_sequences(data, sequence_length, target)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(sequences))\n",
    "train_sequences, test_sequences = sequences[:split_index], sequences[split_index:]\n",
    "train_labels, test_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "# Custom Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_sequences, train_labels)\n",
    "test_dataset = TimeSeriesDataset(test_sequences, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# CNN model for time series classification\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=1):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * (sequence_length // 2), 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Reshape to (batch_size, input_dim, sequence_length) for Conv1d\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = len(features)\n",
    "model = CNNClassifier(input_dim=input_dim)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences).squeeze()\n",
    "            predictions.extend((outputs > 0.5).int().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Train and save the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)\n",
    "torch.save(model.state_dict(), 'cnn_classifier.pth')\n",
    "print(\"Model saved as cnn_classifier.pth\")\n",
    "\n",
    "# Function to load and predict on a new dataset\n",
    "def predict_new_data(file_path, model, scaler):\n",
    "    # Load and preprocess new data\n",
    "    new_data = pd.read_csv(file_path)\n",
    "    new_data[features] = scaler.transform(new_data[features])  # Normalize using the same scaler\n",
    "    \n",
    "    # Prepare sequences for prediction\n",
    "    new_sequences = []\n",
    "    for i in range(len(new_data) - sequence_length):\n",
    "        seq = new_data[features].iloc[i:i+sequence_length].values\n",
    "        new_sequences.append(seq)\n",
    "    new_sequences = np.array(new_sequences)\n",
    "\n",
    "    # Prepare DataLoader for new data\n",
    "    new_dataset = TimeSeriesDataset(new_sequences, np.zeros(len(new_sequences)))  # Use zeros as dummy labels\n",
    "    new_loader = DataLoader(new_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, _ in new_loader:\n",
    "            outputs = model(sequences).squeeze()\n",
    "            predictions = (outputs > 0.5).int().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# Load the saved model\n",
    "model = CNNClassifier(input_dim=input_dim)\n",
    "model.load_state_dict(torch.load('cnn_classifier.pth'))\n",
    "print(\"Model loaded for prediction.\")\n",
    "\n",
    "# Predict on a new CSV file\n",
    "new_predictions = predict_new_data('46.csv', model, scaler)\n",
    "print(\"Predictions on new data:\", new_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2847\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load and preprocess the training data\n",
    "data = pd.read_csv('49_updated.csv')\n",
    "features = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'k', 'm']\n",
    "target = 'group_status'\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Prepare sequences of 10 rows\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(data, sequence_length, target_col):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[features].iloc[i:i+sequence_length].values\n",
    "        label = data[target_col].iloc[i + sequence_length - 1]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequences, labels = create_sequences(data, sequence_length, target)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(sequences))\n",
    "train_sequences, test_sequences = sequences[:split_index], sequences[split_index:]\n",
    "train_labels, test_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "# Custom Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_sequences, train_labels)\n",
    "test_dataset = TimeSeriesDataset(test_sequences, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim=1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        x = self.fc(hn[-1])\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    # Add predict method for permutation importance\n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "            return (self(X).squeeze() > 0.5).int().numpy()\n",
    "\n",
    "# Model parameters\n",
    "input_dim = len(features)\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "model = LSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            predictions.extend((outputs.squeeze() > 0.5).int().numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Define a scikit-learn compatible wrapper for the model\n",
    "# Define a scikit-learn compatible wrapper for the model\n",
    "class SklearnModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # This method doesn't need to do anything, as PyTorch models are trained directly\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Set the model to evaluation mode before prediction\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "            return (self.model(X).squeeze() > 0.5).int().numpy()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return accuracy_score(y, predictions)\n",
    "\n",
    "# Wrap the PyTorch model with the wrapper class\n",
    "sklearn_model = SklearnModelWrapper(model)\n",
    "\n",
    "# Compute feature importance using Permutation Feature Importance\n",
    "def get_feature_importance(model, test_data, test_labels):\n",
    "    # Flatten the 3D test_data to 2D\n",
    "    n_samples, sequence_length, n_features = test_data.shape\n",
    "    test_data_2d = test_data.reshape(n_samples * sequence_length, n_features)\n",
    "    test_labels_repeated = np.repeat(test_labels, sequence_length)\n",
    "    \n",
    "    # Permutation importance on the test data\n",
    "    result = permutation_importance(\n",
    "        model, test_data_2d, test_labels_repeated, n_repeats=10, random_state=42\n",
    "    )\n",
    "    return result.importances_mean\n",
    "\n",
    "# Compute feature importance\n",
    "importance = get_feature_importance(sklearn_model, test_sequences, test_labels)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, importance)\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
